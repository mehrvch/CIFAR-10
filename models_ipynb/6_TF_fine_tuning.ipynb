{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "#include_top = False since we will adapt a dense layer for CIFAR-10 Classification\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(32, 32, 3))  # this assumes K.image_data_format() == 'channels_last'\n",
    "\n",
    "base_model = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:22]:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.chdir('/notebooks/src/CIFAR-10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.load_images import load_data\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "epochs = 1\n",
    "batch_size=32\n",
    "nb_train_samples = X_train.shape[0]\n",
    "nb_validation_samples = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping \n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "X_train,\n",
    "batch_size = batch_size)\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "X_test)\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "# checkpoint = ModelCheckpoint(\"vgg19_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=1562, epochs=1, callbacks=[<keras.ca..., validation_steps=10000)`\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   [ 0.00333718  0.00306036  0.00230681]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  [[ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  [[ 0.00253749  0.00241446  0.00172241]\n   [ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  ..., \n  [[ 0.0023837   0.00186082  0.00159938]\n   [ 0.00267589  0.00219915  0.00178393]\n   [ 0.00249135  0.00204537  0.00167628]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]\n\n  [[ 0.00296809  0.00262976  0.00249135]\n   [ 0.00279892  0.00236832  0.00219915]\n   [ 0.00284506  0.00232218  0.00219915]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]\n\n  [[ 0.00290657  0.00267589  0.00222991]\n   [ 0.00301423  0.00242983  0.00218378]\n   [ 0.00315263  0.00262976  0.00244521]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]]\n\n\n [[[ 0.00189158  0.00207612  0.00178393]\n   [ 0.0018762   0.00202999  0.00179931]\n   [ 0.00158401  0.00169166  0.00156863]\n   ..., \n   [ 0.00039985  0.00059977  0.00063053]\n   [ 0.00055363  0.00076894  0.00075356]\n   [ 0.00055363  0.00076894  0.00075356]]\n\n  [[ 0.00213764  0.00233756  0.00193772]\n   [ 0.00213764  0.00233756  0.00193772]\n   [ 0.00190696  0.00199923  0.00184544]\n   ..., \n   [ 0.00038447  0.00061515  0.00056901]\n   [ 0.00055363  0.00075356  0.00073818]\n   [ 0.0005075   0.00073818  0.00066128]]\n\n  [[ 0.00212226  0.00236832  0.00186082]\n   [ 0.00215302  0.00229143  0.00193772]\n   [ 0.00215302  0.00229143  0.00193772]\n   ..., \n   [ 0.00055363  0.00073818  0.00073818]\n   [ 0.00053825  0.00073818  0.00069204]\n   [ 0.00053825  0.00073818  0.00069204]]\n\n  ..., \n  [[ 0.00186082  0.0020915   0.00143022]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.00173779  0.00192234  0.00126105]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]\n\n  [[ 0.00186082  0.0020915   0.00143022]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.0016609   0.0018762   0.00127643]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]\n\n  [[ 0.00173779  0.00192234  0.00126105]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.0016609   0.0018762   0.00127643]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]]\n\n\n [[[ 0.0028143   0.00258362  0.00224529]\n   [ 0.00175317  0.00167628  0.00147636]\n   [ 0.0014456   0.00146098  0.00139946]\n   ..., \n   [ 0.00192234  0.0018762   0.00184544]\n   [ 0.00192234  0.0018762   0.00184544]\n   [ 0.00192234  0.0018762   0.00184544]]\n\n  [[ 0.00287582  0.00272203  0.00261438]\n   [ 0.00181469  0.00176855  0.00170704]\n   [ 0.00103037  0.00101499  0.00103037]\n   ..., \n   [ 0.00204537  0.00189158  0.0018762 ]\n   [ 0.00204537  0.00189158  0.0018762 ]\n   [ 0.00204537  0.00189158  0.0018762 ]]\n\n  [[ 0.00241446  0.00244521  0.002599  ]\n   [ 0.00163014  0.00164552  0.00179931]\n   [ 0.00126105  0.00113802  0.00129181]\n   ..., \n   [ 0.00252211  0.00172241  0.00183007]\n   [ 0.00252211  0.00172241  0.00183007]\n   [ 0.00252211  0.00172241  0.00183007]]\n\n  ..., \n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]\n\n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]\n\n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]]\n\n\n ..., \n [[[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]]\n\n  [[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]]\n\n  [[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00086121  0.00247597  0.00347559]\n   [ 0.00086121  0.00247597  0.00347559]\n   [ 0.00086121  0.00247597  0.00347559]]\n\n  ..., \n  [[ 0.00164552  0.00293733  0.00381392]\n   [ 0.00164552  0.00256824  0.00333718]\n   [ 0.00099962  0.00189158  0.00262976]\n   ..., \n   [ 0.0013687   0.00278355  0.0037524 ]\n   [ 0.00138408  0.00278355  0.00364475]\n   [ 0.00124567  0.00219915  0.00282968]]\n\n  [[ 0.0016609   0.00296809  0.0038293 ]\n   [ 0.00153787  0.00266052  0.00350634]\n   [ 0.00099962  0.00189158  0.00262976]\n   ..., \n   [ 0.00147636  0.00273741  0.00356786]\n   [ 0.00116878  0.00176855  0.00226067]\n   [ 0.00087659  0.00146098  0.00199923]]\n\n  [[ 0.00164552  0.00296809  0.0038293 ]\n   [ 0.00164552  0.00299885  0.0038293 ]\n   [ 0.00143022  0.00262976  0.00349097]\n   ..., \n   [ 0.00159938  0.00273741  0.00352172]\n   [ 0.00126105  0.00183007  0.00230681]\n   [ 0.00087659  0.00163014  0.00232218]]]\n\n\n [[[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.00073818]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]]\n\n  [[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]]\n\n  [[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00293733  0.00169166  0.00063053]]\n\n  ..., \n  [[ 0.0033218   0.0020915   0.00084583]\n   [ 0.00321415  0.0020915   0.00078431]\n   [ 0.00309112  0.0020915   0.00129181]\n   ..., \n   [ 0.00356786  0.00224529  0.00087659]\n   [ 0.00356786  0.00224529  0.00087659]\n   [ 0.00356786  0.00224529  0.00087659]]\n\n  [[ 0.00315263  0.00192234  0.00064591]\n   [ 0.00298347  0.00179931  0.00059977]\n   [ 0.00284506  0.00172241  0.00090734]\n   ..., \n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00356786  0.00224529  0.00087659]]\n\n  [[ 0.00321415  0.0019531   0.00061515]\n   [ 0.00319877  0.00193772  0.00067666]\n   [ 0.00312188  0.0018762   0.00076894]\n   ..., \n   [ 0.00333718  0.00207612  0.00078431]\n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00350634  0.00219915  0.00086121]]]\n\n\n [[[ 0.00107651  0.00118416  0.00101499]\n   [ 0.00107651  0.00118416  0.00101499]\n   [ 0.00113802  0.00126105  0.00109189]\n   ..., \n   [ 0.00379854  0.00379854  0.00379854]\n   [ 0.00390619  0.00390619  0.00390619]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  [[ 0.00096886  0.00109189  0.00092272]\n   [ 0.00096886  0.00109189  0.00092272]\n   [ 0.00107651  0.00118416  0.00101499]\n   ..., \n   [ 0.00361399  0.00361399  0.00361399]\n   [ 0.00390619  0.00390619  0.00390619]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  [[ 0.00089196  0.00099962  0.00083045]\n   [ 0.00089196  0.00099962  0.00083045]\n   [ 0.00096886  0.00109189  0.00092272]\n   ..., \n   [ 0.00379854  0.00379854  0.00379854]\n   [ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  ..., \n  [[ 0.00184544  0.00172241  0.00118416]\n   [ 0.00175317  0.00163014  0.00107651]\n   [ 0.00184544  0.0016609   0.00113802]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]\n\n  [[ 0.00184544  0.00172241  0.00118416]\n   [ 0.00184544  0.0016609   0.00113802]\n   [ 0.00190696  0.00169166  0.00116878]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]\n\n  [[ 0.00175317  0.00163014  0.00107651]\n   [ 0.00184544  0.0016609   0.00113802]\n   [ 0.00190696  0.00169166  0.00116878]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-9aba0d341c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2022\u001b[0m                                          \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                                          \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m                                          str(generator_output))\n\u001b[0m\u001b[1;32m   2025\u001b[0m                     \u001b[0;31m# build batch logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                     \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   [ 0.00333718  0.00306036  0.00230681]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  [[ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  [[ 0.00253749  0.00241446  0.00172241]\n   [ 0.00329104  0.00335256  0.0023837 ]\n   [ 0.00341407  0.00319877  0.0018762 ]\n   ..., \n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]\n   [ 0.00190696  0.00219915  0.00109189]]\n\n  ..., \n  [[ 0.0023837   0.00186082  0.00159938]\n   [ 0.00267589  0.00219915  0.00178393]\n   [ 0.00249135  0.00204537  0.00167628]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]\n\n  [[ 0.00296809  0.00262976  0.00249135]\n   [ 0.00279892  0.00236832  0.00219915]\n   [ 0.00284506  0.00232218  0.00219915]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]\n\n  [[ 0.00290657  0.00267589  0.00222991]\n   [ 0.00301423  0.00242983  0.00218378]\n   [ 0.00315263  0.00262976  0.00244521]\n   ..., \n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]\n   [ 0.00247597  0.00230681  0.0016609 ]]]\n\n\n [[[ 0.00189158  0.00207612  0.00178393]\n   [ 0.0018762   0.00202999  0.00179931]\n   [ 0.00158401  0.00169166  0.00156863]\n   ..., \n   [ 0.00039985  0.00059977  0.00063053]\n   [ 0.00055363  0.00076894  0.00075356]\n   [ 0.00055363  0.00076894  0.00075356]]\n\n  [[ 0.00213764  0.00233756  0.00193772]\n   [ 0.00213764  0.00233756  0.00193772]\n   [ 0.00190696  0.00199923  0.00184544]\n   ..., \n   [ 0.00038447  0.00061515  0.00056901]\n   [ 0.00055363  0.00075356  0.00073818]\n   [ 0.0005075   0.00073818  0.00066128]]\n\n  [[ 0.00212226  0.00236832  0.00186082]\n   [ 0.00215302  0.00229143  0.00193772]\n   [ 0.00215302  0.00229143  0.00193772]\n   ..., \n   [ 0.00055363  0.00073818  0.00073818]\n   [ 0.00053825  0.00073818  0.00069204]\n   [ 0.00053825  0.00073818  0.00069204]]\n\n  ..., \n  [[ 0.00186082  0.0020915   0.00143022]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.00173779  0.00192234  0.00126105]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]\n\n  [[ 0.00186082  0.0020915   0.00143022]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.0016609   0.0018762   0.00127643]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]\n\n  [[ 0.00173779  0.00192234  0.00126105]\n   [ 0.00173779  0.00192234  0.00126105]\n   [ 0.0016609   0.0018762   0.00127643]\n   ..., \n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]\n   [ 0.00173779  0.00193772  0.0014456 ]]]\n\n\n [[[ 0.0028143   0.00258362  0.00224529]\n   [ 0.00175317  0.00167628  0.00147636]\n   [ 0.0014456   0.00146098  0.00139946]\n   ..., \n   [ 0.00192234  0.0018762   0.00184544]\n   [ 0.00192234  0.0018762   0.00184544]\n   [ 0.00192234  0.0018762   0.00184544]]\n\n  [[ 0.00287582  0.00272203  0.00261438]\n   [ 0.00181469  0.00176855  0.00170704]\n   [ 0.00103037  0.00101499  0.00103037]\n   ..., \n   [ 0.00204537  0.00189158  0.0018762 ]\n   [ 0.00204537  0.00189158  0.0018762 ]\n   [ 0.00204537  0.00189158  0.0018762 ]]\n\n  [[ 0.00241446  0.00244521  0.002599  ]\n   [ 0.00163014  0.00164552  0.00179931]\n   [ 0.00126105  0.00113802  0.00129181]\n   ..., \n   [ 0.00252211  0.00172241  0.00183007]\n   [ 0.00252211  0.00172241  0.00183007]\n   [ 0.00252211  0.00172241  0.00183007]]\n\n  ..., \n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]\n\n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]\n\n  [[ 0.00164552  0.00152249  0.0013687 ]\n   [ 0.00163014  0.00146098  0.00132257]\n   [ 0.00167628  0.00149173  0.0013687 ]\n   ..., \n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]\n   [ 0.0019531   0.00167628  0.00147636]]]\n\n\n ..., \n [[[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]]\n\n  [[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]\n   [ 0.00089196  0.00244521  0.00346021]]\n\n  [[ 0.00124567  0.00276817  0.00370627]\n   [ 0.00126105  0.00278355  0.00369089]\n   [ 0.00127643  0.00278355  0.00369089]\n   ..., \n   [ 0.00086121  0.00247597  0.00347559]\n   [ 0.00086121  0.00247597  0.00347559]\n   [ 0.00086121  0.00247597  0.00347559]]\n\n  ..., \n  [[ 0.00164552  0.00293733  0.00381392]\n   [ 0.00164552  0.00256824  0.00333718]\n   [ 0.00099962  0.00189158  0.00262976]\n   ..., \n   [ 0.0013687   0.00278355  0.0037524 ]\n   [ 0.00138408  0.00278355  0.00364475]\n   [ 0.00124567  0.00219915  0.00282968]]\n\n  [[ 0.0016609   0.00296809  0.0038293 ]\n   [ 0.00153787  0.00266052  0.00350634]\n   [ 0.00099962  0.00189158  0.00262976]\n   ..., \n   [ 0.00147636  0.00273741  0.00356786]\n   [ 0.00116878  0.00176855  0.00226067]\n   [ 0.00087659  0.00146098  0.00199923]]\n\n  [[ 0.00164552  0.00296809  0.0038293 ]\n   [ 0.00164552  0.00299885  0.0038293 ]\n   [ 0.00143022  0.00262976  0.00349097]\n   ..., \n   [ 0.00159938  0.00273741  0.00352172]\n   [ 0.00126105  0.00183007  0.00230681]\n   [ 0.00087659  0.00163014  0.00232218]]]\n\n\n [[[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.00073818]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]]\n\n  [[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00290657  0.0016609   0.00059977]\n   [ 0.00290657  0.0016609   0.00059977]]\n\n  [[ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   [ 0.00307574  0.00179931  0.0007228 ]\n   ..., \n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00293733  0.00169166  0.00063053]\n   [ 0.00293733  0.00169166  0.00063053]]\n\n  ..., \n  [[ 0.0033218   0.0020915   0.00084583]\n   [ 0.00321415  0.0020915   0.00078431]\n   [ 0.00309112  0.0020915   0.00129181]\n   ..., \n   [ 0.00356786  0.00224529  0.00087659]\n   [ 0.00356786  0.00224529  0.00087659]\n   [ 0.00356786  0.00224529  0.00087659]]\n\n  [[ 0.00315263  0.00192234  0.00064591]\n   [ 0.00298347  0.00179931  0.00059977]\n   [ 0.00284506  0.00172241  0.00090734]\n   ..., \n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00356786  0.00224529  0.00087659]]\n\n  [[ 0.00321415  0.0019531   0.00061515]\n   [ 0.00319877  0.00193772  0.00067666]\n   [ 0.00312188  0.0018762   0.00076894]\n   ..., \n   [ 0.00333718  0.00207612  0.00078431]\n   [ 0.00350634  0.00219915  0.00086121]\n   [ 0.00350634  0.00219915  0.00086121]]]\n\n\n [[[ 0.00107651  0.00118416  0.00101499]\n   [ 0.00107651  0.00118416  0.00101499]\n   [ 0.00113802  0.00126105  0.00109189]\n   ..., \n   [ 0.00379854  0.00379854  0.00379854]\n   [ 0.00390619  0.00390619  0.00390619]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  [[ 0.00096886  0.00109189  0.00092272]\n   [ 0.00096886  0.00109189  0.00092272]\n   [ 0.00107651  0.00118416  0.00101499]\n   ..., \n   [ 0.00361399  0.00361399  0.00361399]\n   [ 0.00390619  0.00390619  0.00390619]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  [[ 0.00089196  0.00099962  0.00083045]\n   [ 0.00089196  0.00099962  0.00083045]\n   [ 0.00096886  0.00109189  0.00092272]\n   ..., \n   [ 0.00379854  0.00379854  0.00379854]\n   [ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]]\n\n  ..., \n  [[ 0.00184544  0.00172241  0.00118416]\n   [ 0.00175317  0.00163014  0.00107651]\n   [ 0.00184544  0.0016609   0.00113802]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]\n\n  [[ 0.00184544  0.00172241  0.00118416]\n   [ 0.00184544  0.0016609   0.00113802]\n   [ 0.00190696  0.00169166  0.00116878]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]\n\n  [[ 0.00175317  0.00163014  0.00107651]\n   [ 0.00184544  0.0016609   0.00113802]\n   [ 0.00190696  0.00169166  0.00116878]\n   ..., \n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]\n   [ 0.00204537  0.00181469  0.00133795]]]]"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# # Fit the model on the batches generated by datagen.flow().\n",
    "# model_info = model.fit(X_train, y_train,\n",
    "#                     epochs=epoch,\n",
    "#                     validation_data=(X_test, y_test))\n",
    "\n",
    "# end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 20,559,946\n",
      "Trainable params: 535,562\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # at this point, the top layers are well trained and we can start fine-tuning\n",
    "# # convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# # and train the remaining top layers.\n",
    "\n",
    "# # let's visualize layer names and layer indices to see how many layers\n",
    "# # we should freeze:\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# # the first 249 layers and unfreeze the rest:\n",
    "# for layer in model.layers[:22]:\n",
    "#    layer.trainable = False\n",
    "# for layer in model.layers[22:]:\n",
    "#    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#     samplewise_center=False,  # set each sample mean to 0\n",
    "#     featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#     samplewise_std_normalization=False,  # divide each input by its std\n",
    "#     zca_whitening=False,  # apply ZCA whitening\n",
    "#     rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#     width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#     horizontal_flip=True,  # randomly flip images\n",
    "#     vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# # Compute quantities required for feature-wise normalization\n",
    "# # (std, mean, and principal components if ZCA whitening is applied).\n",
    "# datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we need to recompile the model for these modifications to take effect\n",
    "# # we use SGD with a low learning rate\n",
    "# from keras.optimizers import SGD, rmsprop\n",
    "# # RMSprop optimizer\n",
    "# opt = rmsprop(lr=0.0001, decay=1e-6)\n",
    "# # SGD optimizer\n",
    "# # opt = SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "# # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# # alongside the top Dense layers\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# # model_info = model.fit_generator(generator(X_train, y_train, batch_size=32),\n",
    "# #                     epochs=epoch,\n",
    "# #                     validation_data=(X_test, y_test))\n",
    "# # Fit the model on the batches generated by datagen.flow().\n",
    "# model_info = model.fit_generator(datagen.flow(X_train, y_train,\n",
    "#                                  batch_size=32),\n",
    "#                     steps_per_epoch=X_train.shape[0] // 32,\n",
    "#                     epochs=1,\n",
    "#                     validation_data=(X_test, y_test))\n",
    "# end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result, axis=1)\n",
    "    true_class = np.argmax(test_y, axis=1)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
