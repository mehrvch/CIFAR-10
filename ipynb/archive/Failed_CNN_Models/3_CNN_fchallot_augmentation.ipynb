{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/notebooks/src/CIFAR-10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Convolution2D, Dense, Activation, Flatten,merge\n",
    "from keras.layers import  MaxPooling2D, Dropout, LocallyConnected2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from lib.load_images import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Activation, Dense, Flatten, merge\n",
    "from keras.models import Sequential, Model\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import layers as l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50000, 32, 32, 3))\n",
      "(50000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cnn_3_aug.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]),\n",
    "    \n",
    "    Activation('relu'),\n",
    "    Conv2D(32, (3, 3)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(.25),\n",
    "    Conv2D(64, (3, 3), padding='same'),\n",
    "    Activation('relu'),\n",
    "    Conv2D(64, (3, 3)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 411s - loss: 1.8561 - acc: 0.3133 - val_loss: 1.5386 - val_acc: 0.4463\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 550s - loss: 1.5511 - acc: 0.4340 - val_loss: 1.4160 - val_acc: 0.4961\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 550s - loss: 1.1222 - acc: 0.6045 - val_loss: 0.9600 - val_acc: 0.6653\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 549s - loss: 1.0467 - acc: 0.6313 - val_loss: 0.9245 - val_acc: 0.6779\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 549s - loss: 0.8785 - acc: 0.6966 - val_loss: 0.8015 - val_acc: 0.7257\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 546s - loss: 0.8744 - acc: 0.6975 - val_loss: 0.7372 - val_acc: 0.7469\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 550s - loss: 0.8639 - acc: 0.7015 - val_loss: 0.7381 - val_acc: 0.7439\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 550s - loss: 0.8632 - acc: 0.7033 - val_loss: 0.7456 - val_acc: 0.7474\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 547s - loss: 0.8583 - acc: 0.7071 - val_loss: 0.7050 - val_acc: 0.7602\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 665s - loss: 0.8427 - acc: 0.7106 - val_loss: 0.7072 - val_acc: 0.7577\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 835s - loss: 0.8263 - acc: 0.7179 - val_loss: 0.7136 - val_acc: 0.7549\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 837s - loss: 0.8248 - acc: 0.7182 - val_loss: 0.7261 - val_acc: 0.7609\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 833s - loss: 0.8217 - acc: 0.7194 - val_loss: 0.6707 - val_acc: 0.7682\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 834s - loss: 0.8137 - acc: 0.7228 - val_loss: 0.7095 - val_acc: 0.7569\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 836s - loss: 0.8077 - acc: 0.7227 - val_loss: 0.6713 - val_acc: 0.7753\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 832s - loss: 0.8044 - acc: 0.7265 - val_loss: 0.7055 - val_acc: 0.7601\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 835s - loss: 0.8003 - acc: 0.7263 - val_loss: 0.7012 - val_acc: 0.7689\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 839s - loss: 0.7968 - acc: 0.7290 - val_loss: 0.6859 - val_acc: 0.7748\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 835s - loss: 0.7955 - acc: 0.7281 - val_loss: 0.6613 - val_acc: 0.7734\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 546s - loss: 0.7835 - acc: 0.7345 - val_loss: 0.6601 - val_acc: 0.7766\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 546s - loss: 0.7837 - acc: 0.7341 - val_loss: 0.6634 - val_acc: 0.7744\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7800 - acc: 0.7357 - val_loss: 0.6465 - val_acc: 0.7837\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 550s - loss: 0.7723 - acc: 0.7399 - val_loss: 0.7029 - val_acc: 0.7690\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 551s - loss: 0.7742 - acc: 0.7380 - val_loss: 0.6456 - val_acc: 0.7839\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 550s - loss: 0.7776 - acc: 0.7387 - val_loss: 0.6691 - val_acc: 0.7792\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 551s - loss: 0.7647 - acc: 0.7408 - val_loss: 0.6849 - val_acc: 0.7689\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 544s - loss: 0.7509 - acc: 0.7480 - val_loss: 0.6421 - val_acc: 0.7873\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7579 - acc: 0.7466 - val_loss: 0.6473 - val_acc: 0.7871\n",
      "Epoch 63/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7575 - acc: 0.7476 - val_loss: 0.6406 - val_acc: 0.7877\n",
      "Epoch 64/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7506 - acc: 0.7494 - val_loss: 0.6263 - val_acc: 0.7949\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7566 - acc: 0.7476 - val_loss: 0.6647 - val_acc: 0.7881\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 539s - loss: 0.7519 - acc: 0.7499 - val_loss: 0.6394 - val_acc: 0.7892\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7515 - acc: 0.7499 - val_loss: 0.6058 - val_acc: 0.8004\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7575 - acc: 0.7486 - val_loss: 0.6387 - val_acc: 0.7911\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7564 - acc: 0.7489 - val_loss: 0.6790 - val_acc: 0.7791\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7533 - acc: 0.7493 - val_loss: 0.6435 - val_acc: 0.7852\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 540s - loss: 0.7598 - acc: 0.7484 - val_loss: 0.6403 - val_acc: 0.7917\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7548 - acc: 0.7486 - val_loss: 0.6756 - val_acc: 0.7859\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7608 - acc: 0.7494 - val_loss: 0.6790 - val_acc: 0.7850\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 540s - loss: 0.7593 - acc: 0.7485 - val_loss: 0.6466 - val_acc: 0.7847\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7568 - acc: 0.7485 - val_loss: 0.6624 - val_acc: 0.7925\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 544s - loss: 0.7611 - acc: 0.7482 - val_loss: 0.7079 - val_acc: 0.7721\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 542s - loss: 0.7639 - acc: 0.7493 - val_loss: 0.7216 - val_acc: 0.7900\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 544s - loss: 0.7655 - acc: 0.7462 - val_loss: 0.6596 - val_acc: 0.7824\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 546s - loss: 0.7675 - acc: 0.7448 - val_loss: 0.6621 - val_acc: 0.7822\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7646 - acc: 0.7468 - val_loss: 0.6447 - val_acc: 0.7864\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 547s - loss: 0.7732 - acc: 0.7451 - val_loss: 0.6333 - val_acc: 0.7918\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 544s - loss: 0.7611 - acc: 0.7487 - val_loss: 0.6812 - val_acc: 0.7748\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 541s - loss: 0.7699 - acc: 0.7454 - val_loss: 0.6666 - val_acc: 0.7820\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 543s - loss: 0.7697 - acc: 0.7441 - val_loss: 0.6312 - val_acc: 0.7903\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7780 - acc: 0.7436 - val_loss: 0.6952 - val_acc: 0.7852\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7737 - acc: 0.7451 - val_loss: 0.6262 - val_acc: 0.7936\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 550s - loss: 0.7777 - acc: 0.7430 - val_loss: 0.6459 - val_acc: 0.7910\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 548s - loss: 0.7734 - acc: 0.7456 - val_loss: 0.6830 - val_acc: 0.7794\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 548s - loss: 0.7905 - acc: 0.7408 - val_loss: 0.6488 - val_acc: 0.7824\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 548s - loss: 0.7943 - acc: 0.7396 - val_loss: 0.6978 - val_acc: 0.7769\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 545s - loss: 0.7867 - acc: 0.7422 - val_loss: 0.6820 - val_acc: 0.7857\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 548s - loss: 0.7988 - acc: 0.7381 - val_loss: 0.6848 - val_acc: 0.7851\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 549s - loss: 0.7986 - acc: 0.7385 - val_loss: 0.7101 - val_acc: 0.7573\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 546s - loss: 0.7929 - acc: 0.7407 - val_loss: 0.7011 - val_acc: 0.7669\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 303s - loss: 1.0343 - acc: 0.6701 - val_loss: 0.9684 - val_acc: 0.6907\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 288s - loss: 1.0490 - acc: 0.6684 - val_loss: 0.9112 - val_acc: 0.6956\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 288s - loss: 1.0499 - acc: 0.6665 - val_loss: 1.1220 - val_acc: 0.6309\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 288s - loss: 1.0663 - acc: 0.6610 - val_loss: 0.9115 - val_acc: 0.7082\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 290s - loss: 1.0671 - acc: 0.6626 - val_loss: 0.8920 - val_acc: 0.6957\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 287s - loss: 1.0760 - acc: 0.6603 - val_loss: 0.9106 - val_acc: 0.7301\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 289s - loss: 1.0743 - acc: 0.6585 - val_loss: 0.9274 - val_acc: 0.6941\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 289s - loss: 1.0780 - acc: 0.6574 - val_loss: 1.0945 - val_acc: 0.6225\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 289s - loss: 1.0841 - acc: 0.6519 - val_loss: 1.2010 - val_acc: 0.6198\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 293s - loss: 1.0998 - acc: 0.6533 - val_loss: 1.1602 - val_acc: 0.6476\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 289s - loss: 1.1016 - acc: 0.6512 - val_loss: 1.2096 - val_acc: 0.6285\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 289s - loss: 1.1027 - acc: 0.6480 - val_loss: 0.9248 - val_acc: 0.6912\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1034 - acc: 0.6522 - val_loss: 0.9328 - val_acc: 0.7029\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1160 - acc: 0.6464 - val_loss: 0.9875 - val_acc: 0.6909\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1195 - acc: 0.6464 - val_loss: 1.1518 - val_acc: 0.6459\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1376 - acc: 0.6394 - val_loss: 1.1235 - val_acc: 0.6480\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 286s - loss: 1.1269 - acc: 0.6420 - val_loss: 1.1845 - val_acc: 0.5981\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1379 - acc: 0.6385 - val_loss: 1.0303 - val_acc: 0.6663\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1428 - acc: 0.6350 - val_loss: 1.0306 - val_acc: 0.6866\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.1491 - acc: 0.6352 - val_loss: 1.0537 - val_acc: 0.6904\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1517 - acc: 0.6366 - val_loss: 0.9854 - val_acc: 0.6964\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 295s - loss: 1.1537 - acc: 0.6327 - val_loss: 1.0789 - val_acc: 0.6559\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 312s - loss: 1.1598 - acc: 0.6325 - val_loss: 1.0387 - val_acc: 0.6628\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.1650 - acc: 0.6324 - val_loss: 1.0170 - val_acc: 0.6906\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.1809 - acc: 0.6274 - val_loss: 0.9409 - val_acc: 0.7077\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.1780 - acc: 0.6270 - val_loss: 1.5761 - val_acc: 0.4767\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1840 - acc: 0.6287 - val_loss: 1.4191 - val_acc: 0.5345\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 283s - loss: 1.1950 - acc: 0.6195 - val_loss: 1.0661 - val_acc: 0.6987\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.1858 - acc: 0.6243 - val_loss: 1.0763 - val_acc: 0.6642\n",
      "Epoch 188/200\n",
      "1562/1562 [==============================] - 283s - loss: 1.1977 - acc: 0.6222 - val_loss: 1.1332 - val_acc: 0.6565\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.1933 - acc: 0.6233 - val_loss: 1.1767 - val_acc: 0.6095\n",
      "Epoch 190/200\n",
      "1562/1562 [==============================] - 286s - loss: 1.2067 - acc: 0.6189 - val_loss: 1.0823 - val_acc: 0.6535\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2079 - acc: 0.6177 - val_loss: 1.0430 - val_acc: 0.6659\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2123 - acc: 0.6161 - val_loss: 1.2298 - val_acc: 0.6232\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2207 - acc: 0.6124 - val_loss: 1.0872 - val_acc: 0.6544\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2163 - acc: 0.6155 - val_loss: 1.1158 - val_acc: 0.6349\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 286s - loss: 1.2400 - acc: 0.6094 - val_loss: 1.3336 - val_acc: 0.5609\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2351 - acc: 0.6083 - val_loss: 1.1508 - val_acc: 0.6151\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2338 - acc: 0.6084 - val_loss: 1.1438 - val_acc: 0.6541\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2511 - acc: 0.6069 - val_loss: 1.1537 - val_acc: 0.6458\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 285s - loss: 1.2598 - acc: 0.6032 - val_loss: 1.3079 - val_acc: 0.5989\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 284s - loss: 1.2592 - acc: 0.6027 - val_loss: 1.1255 - val_acc: 0.6275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45907e2ad0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "cnn.fit_generator(datagen.flow(X_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "cnn.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy = history.history['acc']\n",
    "# val_accuracy = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# with open('accuracy_3_CNN_fchallot_aug.pickle', 'wb') as handle:\n",
    "#     pickle.dump(accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # with open('filename.pickle', 'rb') as handle:\n",
    "# #     b = pickle.load(handle)\n",
    "\n",
    "# with open('val_accuracy_3_CNN_fchallot_aug.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # with open('filename.pickle', 'rb') as handle:\n",
    "# #     b = pickle.load(handle)\n",
    "# with open('loss_3_CNN_fchallot_aug.pickle', 'wb') as handle:\n",
    "#     pickle.dump(loss, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # with open('filename.pickle', 'rb') as handle:\n",
    "# #     b = pickle.load(handle)\n",
    "\n",
    "# with open('val_loss_3_CNN_fchallot_aug.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_loss, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # with open('filename.pickle', 'rb') as handle:\n",
    "# #     b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
